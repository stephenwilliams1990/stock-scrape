import pandas as pd
import re
import numpy as np
!pip install bs4
!pip install selenium
from bs4 import BeautifulSoup
import requests
import ftplib 
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait as wait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, date, timezone
from selenium.common.exceptions import TimeoutException

#PART 1 - Import list of stock symbols on the NASDAQ

#download listed stocks to get symbols
FTP_host = 'ftp.nasdaqtrader.com'
FTP = ftplib.FTP()
FTP.connect(FTP_host)
FTP.login()
FTP.cwd('/symboldirectory')
filename = 'nasdaqlisted.txt'
localfile = open(filename, 'wb')
FTP.retrbinary("RETR nasdaqlisted.txt", localfile.write)
FTP.close()
localfile.close()

#read the downloaded symbols file and format as dataframe
symbols = pd.read_csv('nasdaqlisted.txt', sep='delimiter', header=None, engine='python')
headings = symbols.iloc[0][0].split('|')
symbols[headings] = symbols[0].str.split('|', expand=True)
del symbols[0]
symbols = symbols.iloc[1:].reset_index(drop=True)

print(symbols.head())
#create a sample to reduce run time
sample = symbols.head()

#PART 2 - Extracting financial data from Yahoo Finance

db = pd.DataFrame()
#to capture the stocks for which financial data does not exist
errors = []
i = 0
while i < len(sample):
    # Enter a stock symbol
    index = sample.loc[i, 'Symbol']
    
    # URL links for the various sections we want to extract information from
    url_is = 'https://finance.yahoo.com/quote/' + index + '/financials?p=' + index
    url_bs = 'https://finance.yahoo.com/quote/' + index + '/balance-sheet?p=' + index
    url_cf = 'https://finance.yahoo.com/quote/' + index + '/cash-flow?p=' + index
    leave = 0
    
    for info in [url_is, url_bs, url_cf]:
        df = pd.DataFrame()
        res = requests.get(info)
        soup = BeautifulSoup(res.content,'lxml')
            
        headings = []
        table = soup.find('section', attrs={'data-test':'qsp-financial'})
        if table == None:
            errors.append(i)
            leave = 1
            i += 1
            break
        
        
        #if we don't get a successful response, there may be a short term restriction on our IP address. Simple fix is to put the scraper to rest for a short period.
        check = table.find_all('div')
        if check[-1].text == 'Please try reloading the page.':
            print('Timeout. Rest for 5 minutes')
            time.sleep(300)
        
        #Sometimes the data does not completely load on first attempt so this gives a few opportunities to reload the page and extract the data
        retries = 0
        while retries < 3:
            try:
                for l in table.find_all('span', attrs={'class':'Va(m)'}): 
                    headings.append(l.string) # add each element one by one to the list
                data = table.find_all('div', attrs={'data-test':'fin-row'})
                dateData = table.find('div', attrs={'class':'D(tbhg)'}).find_all('div')
                break
            except AttributeError:
                res = requests.get(info)
                soup = BeautifulSoup(res.content,'lxml')
                table = soup.find('section', attrs={'data-test':'qsp-financial'})
                retries += 1
        
        #Now if we do not find any data, we simply add the stock to the error list and move on to the next stock in the list
        if retries == 3:
            errors.append(i)
            leave = 1
            i += 1
            break
        
        #The below code will place the desired data in a pandas dataframe and arrange it
        ls = [] # Create empty list
        for j in range(len(data)):
            a = data[j].find_all('div')
            b = [ele.text for ele in a if len(ele.find_all('div')) == 0][1:]
            ls.append([x for x in b if x])
            
        df = pd.DataFrame(ls).transpose()
        df.columns = headings
        
        dates = []    
        b = [ele.text for ele in dateData if len(ele.find_all('div')) == 0]
        dates.append([x for x in b if x])
            
        df['date'] = dates[0]
        cols = list(df)
        cols.insert(0, cols.pop(cols.index('date')))
        df = df.loc[:, cols]
        df['stock'] = index
        cols = list(df)
        cols.insert(1, cols.pop(cols.index('stock')))
        df = df.loc[:, cols]
        
        if info == url_is:
            comb = df
        else:
            comb = comb.merge(df, on=['date', 'stock'], how='left')
            
    if leave == 1:
        continue
    else:
        i += 1
        db = pd.concat([db, comb], ignore_index=True)   

db.to_csv('stockScrape.csv')

#PART 3 - Extracting historical returns for stocks from Morningstar

db = pd.DataFrame()
i = 0
chrome_options = Options()
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.binary_location="/usr/bin/google-chrome"

driver = webdriver.Chrome('/usr/local/bin/chromedriver', options=chrome_options)
while i < len(sample):
    
    index = sample.loc[i, 'Symbol']
    
    #there are a variety of stocks in our original list that are not the actual company stock price, rather other issues such as warrants or rights.
    match = re.search(r'\b(right|unit|warrant|subordinated)\b',sample.loc[i, 'Security Name'].lower())
    if match:    
        i+=1
        continue
    
    url = 'https://www.morningstar.com/stocks/xnas/' + index + '/price-fair-value'
    driver.get(url)
    
    html = driver.execute_script('return document.body.innerHTML;')
    soup = BeautifulSoup(html,'lxml')    
    
    #Some stocks won't have any data here so we will skip them
    try:
        if soup.find('div', attrs={'class':'error__heading'}).text == 'Like guarantees of future returns, this page doesn’t exist.':
            i+=1
            continue
    except AttributeError:
        pass
    
    element = wait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'total-table')))
    
    html = driver.execute_script('return document.body.innerHTML;')
    soup = BeautifulSoup(html,'lxml')
    table = soup.find('table', attrs={'class':'total-table'})
    headings = table.find('tr', attrs={'class':'thead'}).find_all('td')
    headings = [e.text.strip() for e in headings]
    returns = table.find_all('tr', attrs={'class':'annual-data-row'})[1].find_all('td')
    returns = [e.text.strip() for e in returns]
    returns = pd.DataFrame(returns).transpose()
    returns.columns = headings
    returns['stock'] = index
    
    db = pd.concat([db, returns], ignore_index=True)   
    i+=1

driver.close()
db.to_csv('stockPriceHistory.csv')

#PART 4 - Extract valuation data for stocks from Morningstar

#scrape valuation data history from morningstar
db = pd.DataFrame()
i = 0
driver = webdriver.Chrome('/usr/local/bin/chromedriver', options=chrome_options)
while i < len(sample):
    
    index = sample.loc[i, 'Symbol']
    #there are a variety of stocks in our original list that are not the actual company stock price, rather other issues such as warrants or rights.
    match = re.search(r'\b(right|unit|warrant|subordinated)\b',sample.loc[i, 'Security Name'].lower())
    if match:    
        i+=1
        continue
    
    url = 'https://www.morningstar.com/stocks/xnas/' + index + '/valuation'
    driver.get(url)
    
    html = driver.execute_script('return document.body.innerHTML;')
    soup = BeautifulSoup(html,'lxml')    
    try:
        #if data doesn't exist for the stock, we skip
        if soup.find('div', attrs={'class':'error__heading'}).text in ['Like guarantees of future returns, this page doesn’t exist.']:
            i+=1
            continue
        #sometimes there is just a loading error. This exception will handle that and reload the page.
        if soup.find('div', attrs={'class':'error__heading'}).text in ['This isn’t working, but it’s not your fault.']:
            continue
        
    except AttributeError:
        pass
    
    try:
        element = wait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'sal-component-ctn.sal-component-bar-chart')))
    except TimeoutException:
        try:
            html = driver.execute_script('return document.body.innerHTML;')
            soup = BeautifulSoup(html,'lxml')    
            #if data doesn't exist for the stock, we skip
            if soup.find('div', attrs={'class':'no-available-data ng-scope'}).text.strip() in ['There is no Valuation data available.']:
                i+=1
                continue
        except AttributeError:
            continue
    
    html = driver.execute_script('return document.body.innerHTML;')
    soup = BeautifulSoup(html,'lxml')
    try:
        if soup.find('div', attrs={'class':'no-available-data ng-scope'}).text.strip() in ['There is no Valuation data available.']:
            i+=1
            continue
    except AttributeError:
        pass
    
    #extract the table headings
    table = soup.find('table', attrs={'class':'report-table ng-isolate-scope'})
    headings = table.find('tr', attrs={'class':'thead ng-scope'}).find_all('td')
    headings = [e.text.strip() for e in headings]
    #extract the data
    returns = table.find_all('tr', attrs={'class':'report-table-row ng-scope'})
    info  = []
    for j in range(len(returns)):
        current = returns[j].find_all('td')
        current  = [e.text.strip() for e in current]
        info.append(current)
    
    returns = pd.DataFrame(info)
    returns.columns = headings
    for col in headings:
        if col in ['5-Yr', 'Index']:
            del returns[col]
    
    returns = pd.DataFrame(returns).transpose().reset_index()
    returns = returns.rename(columns=returns.iloc[0])
    returns = returns.drop(returns.index[0]).reset_index(drop=True)
    returns['stock'] = index
    
    db = pd.concat([db, returns], ignore_index=True)   
    i+=1

db.to_csv('valuationScrape.csv')
driver.close()

#PART 5 - Collate data in the one dataset for analysis

financials = pd.read_csv('stockScrape.csv', index_col=0)

#we are assuming that for all stocks, ttm numbers are equal to end of 2020 numbers. So remove ttm numbers (ttm = trailing twelve months)
financials = financials[financials['date'] != 'ttm'].reset_index(drop=True)

#get an idea of which columns appear in the majority of companies financials
cols = []
for col in financials.columns:
    cols.append(len(financials[financials[col].isna()]) / len(financials))

cols = pd.DataFrame(cols).transpose()
cols.columns = financials.columns
test = cols.transpose().reset_index()
#retain only the information items that are in more than 75% of the companies in the dataset
headings = np.array(test[test[0] < 0.25]['index'])
financialsNew = financials[headings]

#we have over 2000 instances of companies that have their reporting at calendar year end. Enough for our analysis, as this will only give examples that /
#line up with the annual returns sourced from Morningstar
stocks = financialsNew.loc[financialsNew['date'] == '12/31/2019', 'stock'].reset_index(drop=True)
df = financialsNew[financialsNew['stock'].isin(np.array(stocks))].reset_index(drop=True)

df['year'] = pd.to_datetime(df['date'], infer_datetime_format=True)
df['year'] = [x.year for x in df['year']]
df['stock'] = df['stock'].astype('|S').str.decode('utf-8') 

#rearrange columns
cols = list(df)
cols.insert(2, cols.pop(cols.index('year')))
df = df.loc[:, cols]

#now bring in other valuation data for these companies and dates.
valuation = pd.read_csv('valuationScrape.csv', index_col=0)

cols = []
for col in valuation.columns:
    cols.append(len(valuation[valuation[col].isna()]) / len(valuation))

cols = pd.DataFrame(cols).transpose()
cols.columns = valuation.columns
test = cols.transpose().reset_index()
#retain only the information items that are in more than 95% of the companies in the dataset
headings = np.array(test[test[0] < 0.05]['index'])
valNew = valuation[headings]
valNew = valNew.rename(columns={'Calendar':'year'})
valNew['stock'] = valNew['stock'].astype('|S').str.decode('utf-8') 
valNew = valNew[valNew['year'] != 'Current'].reset_index(drop=True)
valNew['year'] = valNew['year'].astype(str).astype(int)

#merge datasets to bring in valuation data
df = df.merge(valNew, on=['year', 'stock'], how='left').fillna(0)
try:
    df['Enterprise Value (Mil)'] = df['Enterprise Value (Mil)'].replace(['#VALUE!'], 0)
except KeyError:
    pass
   
#now bring in stock price annual returns
returns = pd.read_csv('stockPriceHistory.csv', index_col=0)

#unstack data to get it in a form where we can do an easy merge
a = returns.iloc[:, :-2].unstack().reset_index()
for i in range(len(a)):
    a.loc[i, 'stock'] = returns.loc[a.loc[i, 'level_1'], 'stock']

del a['level_1']
a.columns = ['year', 'return', 'stock']
a['stock'] = a['stock'].astype('|S').str.decode('utf-8') 
a['year'] = a['year'].astype(str).astype(int)
a = a[a['year'] > 2015].reset_index(drop=True)
a['return'] = a['return'].fillna(0)

#final merge to bring in annual return data
df = df.merge(a, on=['year', 'stock'], how='left').fillna(0)
